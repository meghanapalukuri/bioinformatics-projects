---
output:
  html_document: default
  pdf_document: default
---
```{r global_options, include=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(fig.align = "center", fig.height = 4, fig.width = 6)
library(tidyverse)
theme_set(theme_bw(base_size = 12))
library(ggthemes)
library(plotROC) # for geom_roc() and calc_auc()
```

## SDS 385 Report
*Meghana V Palukuri mvp549*

**Introduction:** Computational prediction of protein complexes from protein-interaction networks can be achieved using supervised machine learning techniques. This is possible using binary classification by extracting topological features from experimentally characterized complexes as the positive class and from random walks sampled from the network as the negative class. Thus, the overarching goal is to input a sampled subgraph from the network to the trained machine learning model to predict whether it is a complex or not. In this project, we work on analyzing the first step - feature extraction, to determine which topological features are important and need to be extracted to construct the final dataset for training a machine learning model. Further, this pipeline for feature selection and dimensionality reduction will become more important in the future as more features (like biological) are added. 

**Question:** *Which topological features are not useful in differentiating protein complexes from random walks in a network ?*

**Dataset:** Generated by extraction of topological features from known protein complexes and negative complexes modelled as random walks on a human protein interaction network. Train and test data comprise 217 and 756 samples (rows) of protein subgraphs respectively. Both train and test data have 19 columns corresponding to 18 features of a subgraph and 1 column corresponding to the class labels (1 or 0, indicating whether a subgraph is a protein complex or not).

**Features:** 

+ **nodes** - no. of nodes (proteins) in subgraph
+ **dens** - density of the subgraph
+ **sv1, sv2, sv3** - 1st 3 singular values of the adjacency matrix of the subgraph
+ **edge_wt_var, edge_wt_max, edge_wt_mean** - edge weight (protein interaction) statistics, ex: edge_wt_mean - mean of all edge weights in the subgraph
+ **CC_var, CC_mean, CC_max** - clustering coefficient statistics
+ **degree_median, degree_mean, degree_var, degree_max** - degree statistics
+ **DC_var, DC_max, DC_mean** - degree correlation statistics

**Analysis:** Features with overall less variance are considered unimportant and are found using a PCA analysis. This is further validated using a stepwise feature selection model employing logistic regression for classification.

```{r warning=FALSE}
# Reading in train and test data 
train_data <- read.csv(file="res_train_dat_humap_new.csv",header=TRUE,sep=",")
test_data <- read.csv(file="res_test_dat_humap_new.csv",header=TRUE,sep=",")
# Converting labels to categorical variable 
train_data$complex <- factor(train_data$complex)
test_data$complex <- factor(test_data$complex)
```

```{r warning=FALSE}
# Scaling train and test data first with 0 mean and unit variance of the training dataset
suppressMessages(library(dataPreparation))
scales_train <- build_scales(dataSet = train_data,verbose = FALSE)
train_data_scaled <- fastScale(dataSet = train_data,scales = scales_train,verbose = FALSE)
test_data_scaled <- fastScale(dataSet = test_data,scales = scales_train,verbose = FALSE)
```

```{r warning=FALSE}
# PCA on the train dataset
train_data_scaled %>% 
  dplyr::select(-complex) %>%   # remove label column
  prcomp() ->            # doing PCA
  pca_model              # store result as `pca_model`

# Finding percentage variation explained by PCs
perc <- 100*pca_model$sdev^2 / sum(pca_model$sdev^2)

# Cumulative sum of variation explained by PCs
print(cumsum(perc))
```
We can see that 11 PCs or more contribute to more than 99% variance of the data. We will use reduced data with 11 and 13 PCs for classification and observe their performance. 
```{r warning=FALSE}
# Dimension reduction with PCA
suppressMessages(library(gsubfn)) # For returning two variables from a function
# Function that returns reduced dimensionality dataset according to no. of PCs
red_dim_pca <- function(N_pcs,pca_model,train_data_scaled,test_data_scaled){
train_labeled_red <- data.frame(pca_model$x[,1:N_pcs], complex = train_data_scaled$complex)

test_data_unlbl <- test_data_scaled %>% dplyr::select(-complex)
pca_test_data <- as.data.frame(predict(pca_model, newdata = test_data_unlbl))
test_labeled_red <- data.frame(pca_test_data[,1:N_pcs], complex = test_data_scaled$complex)
return(list(train_labeled_red,test_labeled_red))}
```

```{r}
N_pcs = 11
list[train_labeled_red11,test_labeled_red11] = red_dim_pca(N_pcs,pca_model,train_data_scaled,test_data_scaled)

N_pcs = 13
list[train_labeled_red13,test_labeled_red13] = red_dim_pca(N_pcs,pca_model,train_data_scaled,test_data_scaled)
```

```{r}
# Logistic regression functions for training and testing
log_fun_trn <- function(feats,train_dat,name) {
  frmla <- as.formula(paste("complex", paste(feats, collapse = " + "), sep = " ~ "))
  model <- glm(frmla, train_dat, family= binomial)
  return(model)} # Returns the model 

log_fun_test <- function(model,test_dat,name) {
  lin_pred <- predict(model, newdata = test_dat)
  prob <- predict(model, newdata = test_dat, type = "response")
  df_test <- data.frame(pred = lin_pred, prob = prob, lbls = as.numeric(test_dat$complex),model = name)
  return(df_test)} # Returns the test results as a dataframe
```

```{r warning=FALSE}
# Logistic regression classification on original and reduced datasets
name = "Original"
feats <- colnames(train_data_scaled)[1:18]
model_orig <- log_fun_trn(feats,train_data_scaled,name)
df_test_orig <- log_fun_test(model_orig,test_data_scaled,name)

N_pcs = 11
name = paste("reduced_pcs",N_pcs)
feats <- colnames(train_labeled_red11)[1:N_pcs]
model_pca_red <- log_fun_trn(feats,train_labeled_red11,name)
df_pca_red11 <- log_fun_test(model_pca_red,test_labeled_red11,name)

N_pcs = 13
name = paste("reduced_pcs",N_pcs)
feats <- colnames(train_labeled_red13)[1:N_pcs]
model_pca_red <- log_fun_trn(feats,train_labeled_red13,name)
df_pca_red13 <- log_fun_test(model_pca_red,test_labeled_red13,name)
```

```{r warning=FALSE}
# Step wise feature selection using logistic regression
suppressMessages(library(caret))
suppressMessages(library(MASS))

model_fit <- train(complex ~ ., 
                   data = train_data_scaled, 
                   method = "glmStepAIC",trace=FALSE)
coef(summary(model_fit))

df_test_fin <- data.frame(prob = predict(model_fit, test_data_scaled,"prob"), lbls = as.numeric(test_data_scaled$complex),model = "glmstep", pred = "None") %>% mutate(prob = prob.1) %>%
  dplyr::select(-prob.0,-prob.1) # Getting test results to same format as others
```

```{r, warning=FALSE}
# Model evaluation - plotting ROC and comparing performance of reduced and full models
df_all <- rbind(df_test_orig,df_pca_red11,df_pca_red13,df_test_fin)

plt <- ggplot(df_all,aes(d=lbls,m=prob,color=model)) + geom_roc(n.cuts=0) +
  coord_fixed() + scale_color_colorblind() 
print(plt)

# Calculating AUCs with corresponding model names printed out
model_names <- unique(df_all$model)
data_info <- data.frame(model_names,group = order(model_names))
left_join(data_info, calc_auc(plt)) %>%
  dplyr::select(-group, -PANEL) %>%
  arrange(desc(AUC))
```
We can see that using 13 PCs gives us an AUC very similar to a model using all features, while there is quite some loss using only 11 PCs. Interestingly, the logistic regression model with only 9 features outperforms the original model with all features, showing us that feature selection is an important step. 
```{r warning=FALSE}
# Density plot 
df_pca_red13 %>% 
  ggplot(aes(x=pred,y = prob, color=factor(lbls))) + geom_line() + geom_point()+ scale_color_colorblind()
```
We can see that the logistic regression model using reduced data with 13 PCs performs nicely, well separating the two classes. 
```{r warning=FALSE}
# Feature selction using PCA
# converting rotation matrix to data frame - gives components of features along the PCs
rot_dat <- data.frame(
  pca_model$rotation)

# Calculating the % of each feature along the PCs of interest
# say, for n PCs of interest out of ntotal PCs, and components of a feature along each PC as f_pc1, f_pc2, .. f_pcn, .. f_pcntotal , we have % = sqrt(f_pc1^2 + .. f_pcn^2)/sqrt(f_pc1^2 + .. f_pcntotal^2). Here, denominator is one since it is a rotation matrix
# So, if % is high it means the feature contributes more to important PCs, and hence to more of the variance of the data.
f <- function(x,inds_interest){
 perc <- 0 
 for (i in inds_interest){
   perc <- x[i]^2 + perc }  
 return(sqrt(perc))}
```

```{r warning=FALSE}
inds_interest = c(1,2,3,4,5,6,7,8,9,10,11,12,13)
perc_norm_imp_PCs <- apply(rot_dat,1, f,inds_interest)
rot_dat %>% mutate(perc_norm_imp_PCs = perc_norm_imp_PCs, var = row.names(pca_model$rotation)) %>% arrange(desc(perc_norm_imp_PCs)) %>% dplyr::select(perc_norm_imp_PCs,var)
```

The 4 features - DC_max, degree_median,degree_mean, DC_mean correspond to the least important features in the pca feature selection analysis above. Of these 4, the 2 features - DC_max and degree_median are unimportant as they do not appear in the logistic regression feature selection model and hence can be removed from the data without affecting the classification results much. 